2024-04-04 22:39:03,352	INFO usage_lib.py:412 -- Usage stats collection is enabled. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-04-04 22:39:03,352	INFO scripts.py:722 -- [37mLocal node IP[39m: [1m10.13.1.34[22m
2024-04-04 22:39:05,550	SUCC scripts.py:759 -- [32m--------------------[39m
2024-04-04 22:39:05,550	SUCC scripts.py:760 -- [32mRay runtime started.[39m
2024-04-04 22:39:05,550	SUCC scripts.py:761 -- [32m--------------------[39m
2024-04-04 22:39:05,550	INFO scripts.py:763 -- [36mNext steps[39m
2024-04-04 22:39:05,550	INFO scripts.py:766 -- To add another node to this Ray cluster, run
2024-04-04 22:39:05,550	INFO scripts.py:769 -- [1m  ray start --address='10.13.1.34:6379'[22m
2024-04-04 22:39:05,550	INFO scripts.py:778 -- To connect to this Ray cluster:
2024-04-04 22:39:05,550	INFO scripts.py:780 -- [35mimport[39m[26m ray
2024-04-04 22:39:05,550	INFO scripts.py:781 -- ray[35m.[39m[26minit()
2024-04-04 22:39:05,550	INFO scripts.py:793 -- To submit a Ray job using the Ray Jobs CLI:
2024-04-04 22:39:05,550	INFO scripts.py:794 -- [1m  RAY_ADDRESS='http://127.0.0.1:8265' ray job submit --working-dir . -- python my_script.py[22m
2024-04-04 22:39:05,550	INFO scripts.py:803 -- See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html 
2024-04-04 22:39:05,550	INFO scripts.py:807 -- for more information on submitting Ray jobs to the Ray cluster.
2024-04-04 22:39:05,550	INFO scripts.py:812 -- To terminate the Ray runtime, run
2024-04-04 22:39:05,550	INFO scripts.py:813 -- [1m  ray stop[22m
2024-04-04 22:39:05,550	INFO scripts.py:816 -- To view the status of the cluster, use
2024-04-04 22:39:05,550	INFO scripts.py:817 --   [1mray status[22m[26m
2024-04-04 22:39:05,550	INFO scripts.py:821 -- To monitor and debug Ray, view the dashboard at 
2024-04-04 22:39:05,550	INFO scripts.py:822 --   [1m127.0.0.1:8265[22m[26m
2024-04-04 22:39:05,551	INFO scripts.py:829 -- [4mIf connection to the dashboard fails, check your firewall settings and network configuration.[24m
2024-04-04 22:39:05,551	INFO scripts.py:929 -- [36m[1m--block[22m[39m
2024-04-04 22:39:05,551	INFO scripts.py:930 -- This command will now block forever until terminated by a signal.
2024-04-04 22:39:05,551	INFO scripts.py:933 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
2024-04-04 22:39:17,567	WARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
2024-04-04 22:39:18,182	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.13.1.34:6379...
2024-04-04 22:39:18,222	INFO worker.py:1612 -- Connected to Ray cluster. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
we're on seed:  42
2024-04-04 22:39:23,860	WARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2024-04-04 22:39:23,861	WARNING algorithm_config.py:2548 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.
Starting a new experiment run.
/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/air/config.py:803: UserWarning: Setting a `RunConfig.local_dir` is deprecated and will be removed in the future. If you are not using remote storage,set the `RunConfig.storage_path` instead. Otherwise, set the`RAY_AIR_LOCAL_CACHE_DIR` environment variable to control the local cache location.
  warnings.warn(
2024-04-04 22:39:24,046	WARNING deprecation.py:50 -- DeprecationWarning: `build_tf_policy` has been deprecated. This will raise an error in the future!
2024-04-04 22:39:24,049	WARNING deprecation.py:50 -- DeprecationWarning: `build_policy_class` has been deprecated. This will raise an error in the future!
2024-04-04 22:39:24,087	WARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/gymnasium/spaces/box.py:127: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:141: UserWarning: [33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64[0m
  logger.warn(
/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:165: UserWarning: [33mWARN: The obs returned by the `reset()` method is not within the observation space.[0m
  logger.warn(f"{pre} is not within the observation space.")
2024-04-04 22:39:24,138	WARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2024-04-04 22:39:24,190	WARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2024-04-04 22:39:24,190	WARNING algorithm_config.py:2548 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.
== Status ==
Current time: 2024-04-04 22:39:24 (running for 00:00:00.25)
Using FIFO scheduling algorithm.
Logical resource usage: 0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 PENDING)


== Status ==
Current time: 2024-04-04 22:39:29 (running for 00:00:05.29)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 PENDING)


== Status ==
Current time: 2024-04-04 22:39:34 (running for 00:00:10.32)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 PENDING)


== Status ==
Current time: 2024-04-04 22:39:39 (running for 00:00:15.34)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 PENDING)


== Status ==
Current time: 2024-04-04 22:39:44 (running for 00:00:20.36)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 PENDING)


== Status ==
Current time: 2024-04-04 22:39:49 (running for 00:00:25.43)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=35,sampler_results/episode_reward_mean=-17556.285714285714,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=2900,num_env_steps_trained=2900 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:39:54 (running for 00:00:30.44)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=140,sampler_results/episode_reward_mean=-11943.3,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=11600,num_env_steps_trained=11600 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:39:59 (running for 00:00:35.48)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:40:04 (running for 00:00:40.57)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=245,sampler_results/episode_reward_mean=-4443.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=20300,num_env_steps_trained=20300 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:40:09 (running for 00:00:45.63)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=350,sampler_results/episode_reward_mean=-594.6,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=29000,num_env_steps_trained=29000 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:40:14 (running for 00:00:50.66)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=455,sampler_results/episode_reward_mean=1465.7,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=37700,num_env_steps_trained=37700 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:40:19 (running for 00:00:55.74)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:40:24 (running for 00:01:00.75)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=560,sampler_results/episode_reward_mean=2352.3,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=46400,num_env_steps_trained=46400 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:40:29 (running for 00:01:05.77)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=665,sampler_results/episode_reward_mean=2780.2,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=55100,num_env_steps_trained=55100 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:40:35 (running for 00:01:10.82)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=770,sampler_results/episode_reward_mean=2934.2,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=63800,num_env_steps_trained=63800 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:40:40 (running for 00:01:15.86)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:40:45 (running for 00:01:20.92)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=875,sampler_results/episode_reward_mean=2927.6,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=72500,num_env_steps_trained=72500 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:40:50 (running for 00:01:25.97)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=1010,sampler_results/episode_reward_mean=3077.2,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=81200,num_env_steps_trained=81200 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:40:55 (running for 00:01:30.99)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:41:00 (running for 00:01:36.03)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=1115,sampler_results/episode_reward_mean=3198.2,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=89900,num_env_steps_trained=89900 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:41:05 (running for 00:01:41.07)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=1185,sampler_results/episode_reward_mean=3187.2,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=95700,num_env_steps_trained=95700 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:41:10 (running for 00:01:46.12)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=1255,sampler_results/episode_reward_mean=3066.2,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=101500,num_env_steps_trained=101500 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:41:15 (running for 00:01:51.17)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=1360,sampler_results/episode_reward_mean=3205.9,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=110200,num_env_steps_trained=110200 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:41:20 (running for 00:01:56.23)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=1435,sampler_results/episode_reward_mean=3319.2,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=116000,num_env_steps_trained=116000 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:41:25 (running for 00:02:01.33)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:41:30 (running for 00:02:06.36)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=1540,sampler_results/episode_reward_mean=3328.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=124700,num_env_steps_trained=124700 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:41:35 (running for 00:02:11.45)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=1645,sampler_results/episode_reward_mean=3355.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=133400,num_env_steps_trained=133400 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:41:40 (running for 00:02:16.50)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=1715,sampler_results/episode_reward_mean=3210.3,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=139200,num_env_steps_trained=139200 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:41:45 (running for 00:02:21.60)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=1820,sampler_results/episode_reward_mean=3405.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=147900,num_env_steps_trained=147900 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:41:50 (running for 00:02:26.72)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:41:56 (running for 00:02:31.82)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=1955,sampler_results/episode_reward_mean=3344.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=156600,num_env_steps_trained=156600 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:42:01 (running for 00:02:36.91)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=2060,sampler_results/episode_reward_mean=3304.9,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=165300,num_env_steps_trained=165300 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:42:06 (running for 00:02:41.97)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:42:11 (running for 00:02:46.97)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=2165,sampler_results/episode_reward_mean=3383.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=174000,num_env_steps_trained=174000 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:42:16 (running for 00:02:52.06)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=2270,sampler_results/episode_reward_mean=3344.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=182700,num_env_steps_trained=182700 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:42:21 (running for 00:02:57.16)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:42:26 (running for 00:03:02.16)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=2375,sampler_results/episode_reward_mean=3048.6,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=191400,num_env_steps_trained=191400 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:42:31 (running for 00:03:07.23)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=2480,sampler_results/episode_reward_mean=3493.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=200100,num_env_steps_trained=200100 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:42:36 (running for 00:03:12.24)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=2585,sampler_results/episode_reward_mean=3465.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=208800,num_env_steps_trained=208800 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:42:41 (running for 00:03:17.25)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:42:46 (running for 00:03:22.27)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=2690,sampler_results/episode_reward_mean=3556.8,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=217500,num_env_steps_trained=217500 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:42:51 (running for 00:03:27.30)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=2795,sampler_results/episode_reward_mean=3494.1,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=226200,num_env_steps_trained=226200 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:42:56 (running for 00:03:32.35)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=2935,sampler_results/episode_reward_mean=3620.6,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=234900,num_env_steps_trained=234900 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:43:01 (running for 00:03:37.44)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:43:06 (running for 00:03:42.47)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=3040,sampler_results/episode_reward_mean=3526.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=243600,num_env_steps_trained=243600 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:43:11 (running for 00:03:47.54)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=3145,sampler_results/episode_reward_mean=3575.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=252300,num_env_steps_trained=252300 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:43:16 (running for 00:03:52.57)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:43:21 (running for 00:03:57.66)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=3250,sampler_results/episode_reward_mean=3522.7,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=261000,num_env_steps_trained=261000 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:43:26 (running for 00:04:02.71)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=3355,sampler_results/episode_reward_mean=3586.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=269700,num_env_steps_trained=269700 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:43:31 (running for 00:04:07.73)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=3460,sampler_results/episode_reward_mean=3498.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=278400,num_env_steps_trained=278400 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:43:36 (running for 00:04:12.79)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:43:42 (running for 00:04:17.88)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=3565,sampler_results/episode_reward_mean=3652.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=287100,num_env_steps_trained=287100 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:43:47 (running for 00:04:22.93)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=3670,sampler_results/episode_reward_mean=3581.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=295800,num_env_steps_trained=295800 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:43:52 (running for 00:04:27.99)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=3775,sampler_results/episode_reward_mean=3581.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=304500,num_env_steps_trained=304500 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:43:57 (running for 00:04:32.99)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:44:02 (running for 00:04:38.07)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=3910,sampler_results/episode_reward_mean=3724.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=313200,num_env_steps_trained=313200 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:44:07 (running for 00:04:43.16)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=4015,sampler_results/episode_reward_mean=3662.4,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=321900,num_env_steps_trained=321900 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:44:12 (running for 00:04:48.23)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:44:17 (running for 00:04:53.32)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=4120,sampler_results/episode_reward_mean=3729.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=330600,num_env_steps_trained=330600 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:44:22 (running for 00:04:58.32)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=4225,sampler_results/episode_reward_mean=3279.6,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=339300,num_env_steps_trained=339300 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:44:27 (running for 00:05:03.35)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=4335,sampler_results/episode_reward_mean=3573.3,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=348000,num_env_steps_trained=348000 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:44:32 (running for 00:05:08.40)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:44:37 (running for 00:05:13.48)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=4440,sampler_results/episode_reward_mean=3674.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=356700,num_env_steps_trained=356700 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:44:42 (running for 00:05:18.53)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=4545,sampler_results/episode_reward_mean=3554.6,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=365400,num_env_steps_trained=365400 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:44:47 (running for 00:05:23.53)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=4650,sampler_results/episode_reward_mean=3696.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=374100,num_env_steps_trained=374100 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:44:52 (running for 00:05:28.58)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:44:57 (running for 00:05:33.63)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=4755,sampler_results/episode_reward_mean=3650.3,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=382800,num_env_steps_trained=382800 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:45:02 (running for 00:05:38.66)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=4890,sampler_results/episode_reward_mean=3728.4,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=391500,num_env_steps_trained=391500 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:45:07 (running for 00:05:43.68)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:45:12 (running for 00:05:48.73)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=4995,sampler_results/episode_reward_mean=3762.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=400200,num_env_steps_trained=400200 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:45:17 (running for 00:05:53.75)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=5100,sampler_results/episode_reward_mean=3680.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=408900,num_env_steps_trained=408900 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:45:23 (running for 00:05:58.85)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=5205,sampler_results/episode_reward_mean=3564.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=417600,num_env_steps_trained=417600 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:45:28 (running for 00:06:03.87)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:45:33 (running for 00:06:08.88)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=5310,sampler_results/episode_reward_mean=3586.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=426300,num_env_steps_trained=426300 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:45:38 (running for 00:06:13.97)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=5415,sampler_results/episode_reward_mean=3746.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=435000,num_env_steps_trained=435000 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:45:43 (running for 00:06:19.03)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=5520,sampler_results/episode_reward_mean=3702.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=443700,num_env_steps_trained=443700 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:45:48 (running for 00:06:24.08)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:45:53 (running for 00:06:29.12)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=5625,sampler_results/episode_reward_mean=3616.2,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=452400,num_env_steps_trained=452400 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:45:58 (running for 00:06:34.18)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=5730,sampler_results/episode_reward_mean=3696.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=461100,num_env_steps_trained=461100 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:46:03 (running for 00:06:39.21)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:46:08 (running for 00:06:44.28)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=5870,sampler_results/episode_reward_mean=3691.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=469800,num_env_steps_trained=469800 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:46:13 (running for 00:06:49.28)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=5975,sampler_results/episode_reward_mean=3647.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=478500,num_env_steps_trained=478500 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:46:18 (running for 00:06:54.36)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=6080,sampler_results/episode_reward_mean=3584.3,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=487200,num_env_steps_trained=487200 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:46:23 (running for 00:06:59.42)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:46:28 (running for 00:07:04.49)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=6185,sampler_results/episode_reward_mean=3597.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=495900,num_env_steps_trained=495900 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:46:33 (running for 00:07:09.54)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=6290,sampler_results/episode_reward_mean=3713.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=504600,num_env_steps_trained=504600 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:46:38 (running for 00:07:14.57)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=6395,sampler_results/episode_reward_mean=3729.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=513300,num_env_steps_trained=513300 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:46:43 (running for 00:07:19.66)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:46:48 (running for 00:07:24.66)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=6500,sampler_results/episode_reward_mean=3772.4,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=522000,num_env_steps_trained=522000 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:46:53 (running for 00:07:29.68)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=6605,sampler_results/episode_reward_mean=3669.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=530700,num_env_steps_trained=530700 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:46:58 (running for 00:07:34.72)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=6710,sampler_results/episode_reward_mean=3735.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=539400,num_env_steps_trained=539400 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:47:03 (running for 00:07:39.76)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:47:08 (running for 00:07:44.80)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=6845,sampler_results/episode_reward_mean=3801.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=548100,num_env_steps_trained=548100 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:47:14 (running for 00:07:49.85)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=6950,sampler_results/episode_reward_mean=3744.9,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=556800,num_env_steps_trained=556800 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:47:19 (running for 00:07:54.85)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:47:24 (running for 00:07:59.91)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=7055,sampler_results/episode_reward_mean=3801.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=565500,num_env_steps_trained=565500 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:47:29 (running for 00:08:04.94)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=7160,sampler_results/episode_reward_mean=3718.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=574200,num_env_steps_trained=574200 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:47:34 (running for 00:08:10.02)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=7270,sampler_results/episode_reward_mean=3757.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=582900,num_env_steps_trained=582900 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:47:39 (running for 00:08:15.04)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:47:44 (running for 00:08:20.12)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=7375,sampler_results/episode_reward_mean=3732.8,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=591600,num_env_steps_trained=591600 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:47:49 (running for 00:08:25.13)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=7480,sampler_results/episode_reward_mean=3751.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=600300,num_env_steps_trained=600300 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:47:54 (running for 00:08:30.14)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:47:59 (running for 00:08:35.16)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=7585,sampler_results/episode_reward_mean=3762.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=609000,num_env_steps_trained=609000 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:48:04 (running for 00:08:40.18)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=7655,sampler_results/episode_reward_mean=3658.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=614800,num_env_steps_trained=614800 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:48:09 (running for 00:08:45.23)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=7755,sampler_results/episode_reward_mean=3817.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=620600,num_env_steps_trained=620600 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:48:14 (running for 00:08:50.31)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=7825,sampler_results/episode_reward_mean=3746.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=626400,num_env_steps_trained=626400 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:48:19 (running for 00:08:55.37)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=7895,sampler_results/episode_reward_mean=3790.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=632200,num_env_steps_trained=632200 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:48:24 (running for 00:09:00.40)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=7965,sampler_results/episode_reward_mean=3784.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=638000,num_env_steps_trained=638000 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:48:29 (running for 00:09:05.47)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=8035,sampler_results/episode_reward_mean=3702.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=643800,num_env_steps_trained=643800 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:48:34 (running for 00:09:10.55)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=8105,sampler_results/episode_reward_mean=3669.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=649600,num_env_steps_trained=649600 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:48:39 (running for 00:09:15.60)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=8175,sampler_results/episode_reward_mean=3669.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=655400,num_env_steps_trained=655400 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:48:44 (running for 00:09:20.62)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=8245,sampler_results/episode_reward_mean=3516.1,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=661200,num_env_steps_trained=661200 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:48:49 (running for 00:09:25.70)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=8350,sampler_results/episode_reward_mean=3762.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=669900,num_env_steps_trained=669900 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:48:54 (running for 00:09:30.72)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=8455,sampler_results/episode_reward_mean=3817.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=678600,num_env_steps_trained=678600 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:48:59 (running for 00:09:35.73)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:49:04 (running for 00:09:40.75)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=8560,sampler_results/episode_reward_mean=3796.6,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=687300,num_env_steps_trained=687300 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:49:09 (running for 00:09:45.79)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=8700,sampler_results/episode_reward_mean=3812.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=696000,num_env_steps_trained=696000 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:49:15 (running for 00:09:50.87)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:49:20 (running for 00:09:55.91)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=8805,sampler_results/episode_reward_mean=3872.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=704700,num_env_steps_trained=704700 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:49:25 (running for 00:10:00.93)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=8910,sampler_results/episode_reward_mean=3836.2,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=713400,num_env_steps_trained=713400 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:49:30 (running for 00:10:05.94)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=9015,sampler_results/episode_reward_mean=3751.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=722100,num_env_steps_trained=722100 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:49:35 (running for 00:10:10.97)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:49:40 (running for 00:10:16.05)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=9120,sampler_results/episode_reward_mean=3845.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=730800,num_env_steps_trained=730800 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:49:45 (running for 00:10:21.09)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=9225,sampler_results/episode_reward_mean=3806.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=739500,num_env_steps_trained=739500 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:49:50 (running for 00:10:26.11)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=9330,sampler_results/episode_reward_mean=3828.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=748200,num_env_steps_trained=748200 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:49:55 (running for 00:10:31.20)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:50:00 (running for 00:10:36.24)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=9435,sampler_results/episode_reward_mean=3692.1,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=756900,num_env_steps_trained=756900 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:50:05 (running for 00:10:41.29)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=9540,sampler_results/episode_reward_mean=3845.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=765600,num_env_steps_trained=765600 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:50:10 (running for 00:10:46.33)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=9675,sampler_results/episode_reward_mean=3861.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=774300,num_env_steps_trained=774300 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:50:15 (running for 00:10:51.40)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:50:20 (running for 00:10:56.47)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=9780,sampler_results/episode_reward_mean=3685.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=783000,num_env_steps_trained=783000 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:50:25 (running for 00:11:01.49)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=9885,sampler_results/episode_reward_mean=3570.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=791700,num_env_steps_trained=791700 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:50:31 (running for 00:11:06.94)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:50:36 (running for 00:11:11.98)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=9990,sampler_results/episode_reward_mean=3728.4,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=800400,num_env_steps_trained=800400 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:50:41 (running for 00:11:17.07)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=10095,sampler_results/episode_reward_mean=3806.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=809100,num_env_steps_trained=809100 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:50:46 (running for 00:11:22.17)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=10170,sampler_results/episode_reward_mean=3768.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=814900,num_env_steps_trained=814900 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:50:51 (running for 00:11:27.18)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=10275,sampler_results/episode_reward_mean=3894.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=823600,num_env_steps_trained=823600 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:50:56 (running for 00:11:32.30)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=10345,sampler_results/episode_reward_mean=3894.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=829400,num_env_steps_trained=829400 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
== Status ==
Current time: 2024-04-04 22:51:01 (running for 00:11:37.38)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


== Status ==
Current time: 2024-04-04 22:51:06 (running for 00:11:42.40)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 RUNNING)


Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=10415,sampler_results/episode_reward_mean=3850.5,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=835200,num_env_steps_trained=835200 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}.
Trial PPO_DynamicCoalitionsEnv_6c206_00000 reported episodes_total=10485,sampler_results/episode_reward_mean=3845.0,sampler_results/episode_len_mean=80.0,num_env_steps_sampled=841000,num_env_steps_trained=841000 with parameters={'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': <class 'B_env.DynamicCoalitionsEnv'>, 'env_config': {'num_agents': 5, 'char_func_dict': {'mode': 'ridesharing', 'k': 20, 'alpha': 1}, 'manual_distances': [[0.15, 0.4, 0.1, 0.35, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.4, 0.05, 0.45, 0.45], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.4, 0.1, 0.45, 0.45], [0.2, 0.45, 0.05, 0.4, 0.5], [0.15, 0.5, 0.05, 0.35, 0.45], [0.1, 0.5, 0.05, 0.4, 0.5], [0.15, 0.45, 0.05, 0.45, 0.45], [0.2, 0.5, 0.05, 0.4, 0.5], [0.1, 0.45, 0.05, 0.4, 0.5], [0.05, 0.5, 0.1, 0.35, 0.45]], 'max_steps': 10000, 'batch_size': 2900}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.5, 'lr': 0.00025, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 2900, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [400, 400], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x149a03ab5160>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 42, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': [[0, 0.00025], [50000000, 2.5e-05]], 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.5, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.03, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy0': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy1': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy2': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy3': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None), 'policy4': (None, Dict('coalitions': Box(0, 1, (2, 5), int32), 'distances': Box(0.0, 100.0, (2, 5), float32)), Discrete(2), None)}, 'callbacks': <class 'C_ppo_config.Custom_callback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 35}. This trial completed.
== Status ==
Current time: 2024-04-04 22:51:11 (running for 00:11:47.18)
Using FIFO scheduling algorithm.
Logical resource usage: 36.0/96 CPUs, 0/0 GPUs
Result logdir: /p/home/jusers/cipolina-kun1/juwels/ray_results/april_five
Number of trials: 1/1 (1 TERMINATED)
+--------------------------------------+------------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name                           | status     | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|--------------------------------------+------------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_DynamicCoalitionsEnv_6c206_00000 | TERMINATED | 10.13.1.34:17840 |    290 |          667.784 | 841000 |     3845 |                 3900 |                 2800 |                 80 |
+--------------------------------------+------------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


2024-04-04 22:51:11,390	INFO tune.py:1148 -- Total run time: 707.46 seconds (707.15 seconds for the tuning loop).
BEST ITERATION:
mean_reward_policy0: 146.0
mean_reward_policy1: 151.42857142857142
mean_reward_policy2: 151.42857142857142
mean_reward_policy3: 146.85714285714286
mean_reward_policy4: 154.85714285714286
     training_iteration  episode_len_mean  episode_reward_mean
0                     1              80.0        -17556.285714
1                     2              80.0        -16445.285714
2                     3              80.0        -14780.200000
3                     4              80.0        -11943.300000
4                     5              80.0         -9171.300000
..                  ...               ...                  ...
285                 286              80.0          3894.500000
286                 287              80.0          3867.000000
287                 288              80.0          3850.500000
288                 289              80.0          3845.000000
289                 290              80.0          3845.000000

[290 rows x 3 columns]
/p/home/jusers/cipolina-kun1/juwels/coalitions/E_graphing.py:84: FutureWarning: 

The `ci` parameter is deprecated. Use `errorbar='sd'` for the same effect.

  sns.lineplot(x="training_iteration", y=metric, data=data, ci='sd', label=policy, ax=ax)
/p/home/jusers/cipolina-kun1/juwels/coalitions/E_graphing.py:84: FutureWarning: 

The `ci` parameter is deprecated. Use `errorbar='sd'` for the same effect.

  sns.lineplot(x="training_iteration", y=metric, data=data, ci='sd', label=policy, ax=ax)
/p/home/jusers/cipolina-kun1/juwels/coalitions/E_graphing.py:84: FutureWarning: 

The `ci` parameter is deprecated. Use `errorbar='sd'` for the same effect.

  sns.lineplot(x="training_iteration", y=metric, data=data, ci='sd', label=policy, ax=ax)
/p/home/jusers/cipolina-kun1/juwels/coalitions/E_graphing.py:84: FutureWarning: 

The `ci` parameter is deprecated. Use `errorbar='sd'` for the same effect.

  sns.lineplot(x="training_iteration", y=metric, data=data, ci='sd', label=policy, ax=ax)
/p/home/jusers/cipolina-kun1/juwels/coalitions/E_graphing.py:84: FutureWarning: 

The `ci` parameter is deprecated. Use `errorbar='sd'` for the same effect.

  sns.lineplot(x="training_iteration", y=metric, data=data, ci='sd', label=policy, ax=ax)
/p/home/jusers/cipolina-kun1/juwels/coalitions/E_graphing.py:84: FutureWarning: 

The `ci` parameter is deprecated. Use `errorbar='sd'` for the same effect.

  sns.lineplot(x="training_iteration", y=metric, data=data, ci='sd', label=policy, ax=ax)
/p/home/jusers/cipolina-kun1/juwels/coalitions/E_graphing.py:84: FutureWarning: 

The `ci` parameter is deprecated. Use `errorbar='sd'` for the same effect.

  sns.lineplot(x="training_iteration", y=metric, data=data, ci='sd', label=policy, ax=ax)
/p/home/jusers/cipolina-kun1/juwels/coalitions/E_graphing.py:84: FutureWarning: 

The `ci` parameter is deprecated. Use `errorbar='sd'` for the same effect.

  sns.lineplot(x="training_iteration", y=metric, data=data, ci='sd', label=policy, ax=ax)
/p/home/jusers/cipolina-kun1/juwels/coalitions/E_graphing.py:84: FutureWarning: 

The `ci` parameter is deprecated. Use `errorbar='sd'` for the same effect.

  sns.lineplot(x="training_iteration", y=metric, data=data, ci='sd', label=policy, ax=ax)
/p/home/jusers/cipolina-kun1/juwels/coalitions/E_graphing.py:84: FutureWarning: 

The `ci` parameter is deprecated. Use `errorbar='sd'` for the same effect.

  sns.lineplot(x="training_iteration", y=metric, data=data, ci='sd', label=policy, ax=ax)
/p/home/jusers/cipolina-kun1/juwels/coalitions/E_graphing.py:84: FutureWarning: 

The `ci` parameter is deprecated. Use `errorbar='sd'` for the same effect.

  sns.lineplot(x="training_iteration", y=metric, data=data, ci='sd', label=policy, ax=ax)
/p/home/jusers/cipolina-kun1/juwels/coalitions/E_graphing.py:84: FutureWarning: 

The `ci` parameter is deprecated. Use `errorbar='sd'` for the same effect.

  sns.lineplot(x="training_iteration", y=metric, data=data, ci='sd', label=policy, ax=ax)
/p/home/jusers/cipolina-kun1/juwels/coalitions/E_graphing.py:84: FutureWarning: 

The `ci` parameter is deprecated. Use `errorbar='sd'` for the same effect.

  sns.lineplot(x="training_iteration", y=metric, data=data, ci='sd', label=policy, ax=ax)
/p/home/jusers/cipolina-kun1/juwels/coalitions/E_graphing.py:84: FutureWarning: 

The `ci` parameter is deprecated. Use `errorbar='sd'` for the same effect.

  sns.lineplot(x="training_iteration", y=metric, data=data, ci='sd', label=policy, ax=ax)
/p/home/jusers/cipolina-kun1/juwels/coalitions/E_graphing.py:84: FutureWarning: 

The `ci` parameter is deprecated. Use `errorbar='sd'` for the same effect.

  sns.lineplot(x="training_iteration", y=metric, data=data, ci='sd', label=policy, ax=ax)
2024-04-04 22:51:15,075	INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.13.1.34:6379...
2024-04-04 22:51:15,080	INFO worker.py:1612 -- Connected to Ray cluster. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
2024-04-04 22:51:15,096	WARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2024-04-04 22:51:15,096	WARNING algorithm_config.py:2548 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.
2024-04-04 22:51:15,101	WARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2024-04-04 22:51:15,102	WARNING algorithm_config.py:2548 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.
2024-04-04 22:51:15,102	WARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.
2024-04-04 22:51:15,138	WARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2024-04-04 22:51:15,138	WARNING algorithm_config.py:2548 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.
/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
`UnifiedLogger` will be removed in Ray 2.7.
  return UnifiedLogger(config, logdir, loggers=None)
/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
[2m[36m(pid=22910)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=22906)[0m 2024-04-04 22:51:25,003	WARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
[2m[36m(RolloutWorker pid=22904)[0m 2024-04-04 22:51:25,040	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.complex_input_net.ComplexInputNetwork` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=22904)[0m 2024-04-04 22:51:25,040	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!
[2m[36m(RolloutWorker pid=22904)[0m 2024-04-04 22:51:25,040	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=22904)[0m 2024-04-04 22:51:25,072	WARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=22904)[0m 2024-04-04 22:51:25,072	WARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=22904)[0m 2024-04-04 22:51:25,072	WARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=22904)[0m 2024-04-04 22:51:25,073	WARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=22904)[0m 2024-04-04 22:51:25,073	WARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=22904)[0m 2024-04-04 22:51:25,073	WARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=22904)[0m 2024-04-04 22:51:25,073	WARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=22904)[0m 2024-04-04 22:51:25,078	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!
2024-04-04 22:51:25,225	WARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2024-04-04 22:51:25,227	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.complex_input_net.ComplexInputNetwork` has been deprecated. This will raise an error in the future!
2024-04-04 22:51:25,227	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!
2024-04-04 22:51:25,228	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!
2024-04-04 22:51:25,280	WARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!
2024-04-04 22:51:25,280	WARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!
2024-04-04 22:51:25,281	WARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!
2024-04-04 22:51:25,281	WARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!
2024-04-04 22:51:25,282	WARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!
2024-04-04 22:51:25,282	WARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!
2024-04-04 22:51:25,282	WARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!
2024-04-04 22:51:25,288	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!
2024-04-04 22:51:25,298	WARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2024-04-04 22:51:25,324	WARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2024-04-04 22:51:25,353	WARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2024-04-04 22:51:25,380	WARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2024-04-04 22:51:25,561	INFO trainable.py:172 -- Trainable.setup took 10.213 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2024-04-04 22:51:25,562	WARNING util.py:68 -- Install gputil for GPU system monitoring.
env distance_lst: [0.16, 0.41, 0.11, 0.36, 0.46]
env distance_lst: [0.21, 0.46, 0.06, 0.41, 0.5]
env distance_lst: [0.16, 0.41, 0.06, 0.46, 0.46]
env distance_lst: [0.11, 0.46, 0.06, 0.41, 0.5]
env distance_lst: [0.06, 0.41, 0.11, 0.46, 0.46]
env distance_lst: [0.21, 0.46, 0.06, 0.41, 0.5]
env distance_lst: [0.16, 0.5, 0.06, 0.36, 0.46]
env distance_lst: [0.11, 0.5, 0.06, 0.41, 0.5]
env distance_lst: [0.16, 0.46, 0.06, 0.46, 0.46]
env distance_lst: [0.21, 0.5, 0.06, 0.41, 0.5]
env distance_lst: [0.11, 0.46, 0.06, 0.41, 0.5]
env distance_lst: [0.06, 0.5, 0.11, 0.36, 0.46]
[2m[36m(pid=22909)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future![32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(RolloutWorker pid=22909)[0m 2024-04-04 22:51:25,176	WARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.[32m [repeated 34x across cluster][0m
   Agent  Test Accuracy    Std Dev
0      0      72.791667  14.007893
1      1      85.250000   7.134077
2      2      82.458333   7.467651
3      3      80.833333   8.891059
4      4      83.500000   7.270671
2024-04-04 22:51:55,091	VINFO scripts.py:1068 -- Send termination request to `[1m/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/raylet --store_socket_name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/plasma_store --object_manager_port=0 --min_worker_port=10002 --max_worker_port=19999 --node_manager_port=0 --node_ip_address=10.13.1.34 --maximum_startup_concurrency=96 --static_resource_list=node:10.13.1.34,1.0,node:__internal_head__,1.0,CPU,96,memory,54159185511,object_store_memory,27079592755 "--python_worker_command=/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/bin/python /p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/_private/workers/setup_worker.py /p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/_private/workers/default_worker.py --node-ip-address=10.13.1.34 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/plasma_store --raylet-name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/raylet --redis-address=None --temp-dir=/tmp/ray --metrics-agent-port=46040 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=10.13.1.34:6379 --session-name=session_2024-04-04_22-39-03_353741_14396 --temp-dir=/tmp/ray --webui=127.0.0.1:8265 RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER" --java_worker_command= --cpp_worker_command= --native_library_path=/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/cpp/lib --temp_dir=/tmp/ray --session_dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396 --log_dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/logs --resource_dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/runtime_resources --metrics-agent-port=46040 --metrics_export_port=63228 --object_store_memory=27079592755 --plasma_directory=/dev/shm --ray-debugger-external=0 --gcs-address=10.13.1.34:6379 --session-name=session_2024-04-04_22-39-03_353741_14396 --labels= --head --num_prestart_python_workers=96 "--agent_command=/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/bin/python -u /p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/dashboard/agent.py --node-ip-address=10.13.1.34 --metrics-export-port=63228 --dashboard-agent-port=46040 --listen-port=52365 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/plasma_store --raylet-name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/raylet --temp-dir=/tmp/ray --session-dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396 --runtime-env-dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/runtime_resources --log-dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2024-04-04_22-39-03_353741_14396 --gcs-address=10.13.1.34:6379"[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,111	VINFO scripts.py:1068 -- Send termination request to `[1m/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/bin/python -u /p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/autoscaler/_private/monitor.py --logs-dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=10.13.1.34:6379 --monitor-ip=10.13.1.34[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,111	VINFO scripts.py:1068 -- Send termination request to `[1m/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/bin/python -u /p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/_private/log_monitor.py --logs-dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/logs --gcs-address=10.13.1.34:6379 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,131	VINFO scripts.py:1068 -- Send termination request to `[1m/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/bin/python -m ray.util.client.server --address=10.13.1.34:6379 --host=0.0.0.0 --port=10001 --mode=proxy --metrics-agent-port=46040[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,151	VINFO scripts.py:1068 -- Send termination request to `[1m/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/raylet --store_socket_name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/plasma_store --object_manager_port=0 --min_worker_port=10002 --max_worker_port=19999 --node_manager_port=0 --node_ip_address=10.13.1.34 --maximum_startup_concurrency=96 --static_resource_list=node:10.13.1.34,1.0,node:__internal_head__,1.0,CPU,96,memory,54159185511,object_store_memory,27079592755 "--python_worker_command=/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/bin/python /p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/_private/workers/setup_worker.py /p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/_private/workers/default_worker.py --node-ip-address=10.13.1.34 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/plasma_store --raylet-name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/raylet --redis-address=None --temp-dir=/tmp/ray --metrics-agent-port=46040 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=10.13.1.34:6379 --session-name=session_2024-04-04_22-39-03_353741_14396 --temp-dir=/tmp/ray --webui=127.0.0.1:8265 RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER" --java_worker_command= --cpp_worker_command= --native_library_path=/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/cpp/lib --temp_dir=/tmp/ray --session_dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396 --log_dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/logs --resource_dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/runtime_resources --metrics-agent-port=46040 --metrics_export_port=63228 --object_store_memory=27079592755 --plasma_directory=/dev/shm --ray-debugger-external=0 --gcs-address=10.13.1.34:6379 --session-name=session_2024-04-04_22-39-03_353741_14396 --labels= --head --num_prestart_python_workers=96 "--agent_command=/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/bin/python -u /p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/dashboard/agent.py --node-ip-address=10.13.1.34 --metrics-export-port=63228 --dashboard-agent-port=46040 --listen-port=52365 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/plasma_store --raylet-name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/raylet --temp-dir=/tmp/ray --session-dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396 --runtime-env-dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/runtime_resources --log-dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2024-04-04_22-39-03_353741_14396 --gcs-address=10.13.1.34:6379"[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,170	VINFO scripts.py:1068 -- Send termination request to `[1m/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/raylet --store_socket_name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/plasma_store --object_manager_port=0 --min_worker_port=10002 --max_worker_port=19999 --node_manager_port=0 --node_ip_address=10.13.1.34 --maximum_startup_concurrency=96 --static_resource_list=node:10.13.1.34,1.0,node:__internal_head__,1.0,CPU,96,memory,54159185511,object_store_memory,27079592755 "--python_worker_command=/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/bin/python /p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/_private/workers/setup_worker.py /p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/_private/workers/default_worker.py --node-ip-address=10.13.1.34 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/plasma_store --raylet-name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/raylet --redis-address=None --temp-dir=/tmp/ray --metrics-agent-port=46040 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=10.13.1.34:6379 --session-name=session_2024-04-04_22-39-03_353741_14396 --temp-dir=/tmp/ray --webui=127.0.0.1:8265 RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER" --java_worker_command= --cpp_worker_command= --native_library_path=/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/cpp/lib --temp_dir=/tmp/ray --session_dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396 --log_dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/logs --resource_dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/runtime_resources --metrics-agent-port=46040 --metrics_export_port=63228 --object_store_memory=27079592755 --plasma_directory=/dev/shm --ray-debugger-external=0 --gcs-address=10.13.1.34:6379 --session-name=session_2024-04-04_22-39-03_353741_14396 --labels= --head --num_prestart_python_workers=96 "--agent_command=/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/bin/python -u /p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/dashboard/agent.py --node-ip-address=10.13.1.34 --metrics-export-port=63228 --dashboard-agent-port=46040 --listen-port=52365 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/plasma_store --raylet-name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/raylet --temp-dir=/tmp/ray --session-dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396 --runtime-env-dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/runtime_resources --log-dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2024-04-04_22-39-03_353741_14396 --gcs-address=10.13.1.34:6379"[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,171	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,171	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,172	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,172	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,173	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,173	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,173	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,174	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,174	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,175	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,175	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,176	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,176	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,176	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,177	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,177	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,178	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,178	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,179	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,179	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,179	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,180	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,180	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,181	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,181	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,181	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,182	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,182	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,183	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,183	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,184	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,184	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,184	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,185	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,185	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,186	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,186	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,187	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,187	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,187	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,188	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,188	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,189	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,189	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,190	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,190	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,190	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,191	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,191	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,192	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,192	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,192	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,193	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,193	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,194	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,194	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,195	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,195	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,195	VINFO scripts.py:1068 -- Send termination request to `[1mray::IDLE "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,236	VINFO scripts.py:1068 -- Send termination request to `[1m/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/bin/python -u /p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/_private/log_monitor.py --logs-dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/logs --gcs-address=10.13.1.34:6379 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,284	VINFO scripts.py:1068 -- Send termination request to `[1m/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/raylet --store_socket_name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/plasma_store --object_manager_port=0 --min_worker_port=10002 --max_worker_port=19999 --node_manager_port=0 --node_ip_address=10.13.1.34 --maximum_startup_concurrency=96 --static_resource_list=node:10.13.1.34,1.0,node:__internal_head__,1.0,CPU,96,memory,54159185511,object_store_memory,27079592755 "--python_worker_command=/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/bin/python /p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/_private/workers/setup_worker.py /p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/_private/workers/default_worker.py --node-ip-address=10.13.1.34 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/plasma_store --raylet-name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/raylet --redis-address=None --temp-dir=/tmp/ray --metrics-agent-port=46040 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=10.13.1.34:6379 --session-name=session_2024-04-04_22-39-03_353741_14396 --temp-dir=/tmp/ray --webui=127.0.0.1:8265 RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER" --java_worker_command= --cpp_worker_command= --native_library_path=/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/cpp/lib --temp_dir=/tmp/ray --session_dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396 --log_dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/logs --resource_dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/runtime_resources --metrics-agent-port=46040 --metrics_export_port=63228 --object_store_memory=27079592755 --plasma_directory=/dev/shm --ray-debugger-external=0 --gcs-address=10.13.1.34:6379 --session-name=session_2024-04-04_22-39-03_353741_14396 --labels= --head --num_prestart_python_workers=96 "--agent_command=/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/bin/python -u /p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/dashboard/agent.py --node-ip-address=10.13.1.34 --metrics-export-port=63228 --dashboard-agent-port=46040 --listen-port=52365 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/plasma_store --raylet-name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/raylet --temp-dir=/tmp/ray --session-dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396 --runtime-env-dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/runtime_resources --log-dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2024-04-04_22-39-03_353741_14396 --gcs-address=10.13.1.34:6379"[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,284	VINFO scripts.py:1068 -- Send termination request to `[1m/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/bin/python -u /p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/dashboard/agent.py --node-ip-address=10.13.1.34 --metrics-export-port=63228 --dashboard-agent-port=46040 --listen-port=52365 --node-manager-port=46373 --object-store-name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/plasma_store --raylet-name=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/sockets/raylet --temp-dir=/tmp/ray --session-dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396 --runtime-env-dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/runtime_resources --log-dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2024-04-04_22-39-03_353741_14396 --gcs-address=10.13.1.34:6379 --agent-id 424238335[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,305	VINFO scripts.py:1068 -- Send termination request to `[1m/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/bin/python /p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/dashboard/dashboard.py --host=127.0.0.1 --port=8265 --port-retries=0 --temp-dir=/tmp/ray --log-dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/logs --session-dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=10.13.1.34:6379 --node-ip-address=10.13.1.34[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:55,327	INFO scripts.py:1098 -- 1/65 stopped.2024-04-04 22:51:55,327	INFO scripts.py:1098 -- 2/65 stopped.2024-04-04 22:51:55,327	INFO scripts.py:1098 -- 3/65 stopped.2024-04-04 22:51:55,327	INFO scripts.py:1098 -- 4/65 stopped.2024-04-04 22:51:55,327	INFO scripts.py:1098 -- 5/65 stopped.2024-04-04 22:51:55,327	INFO scripts.py:1098 -- 6/65 stopped.2024-04-04 22:51:55,327	INFO scripts.py:1098 -- 7/65 stopped.2024-04-04 22:51:55,327	INFO scripts.py:1098 -- 8/65 stopped.2024-04-04 22:51:55,327	INFO scripts.py:1098 -- 9/65 stopped.2024-04-04 22:51:55,327	INFO scripts.py:1098 -- 10/65 stopped.2024-04-04 22:51:55,327	INFO scripts.py:1098 -- 11/65 stopped.2024-04-04 22:51:55,327	INFO scripts.py:1098 -- 12/65 stopped.2024-04-04 22:51:55,327	INFO scripts.py:1098 -- 13/65 stopped.2024-04-04 22:51:55,327	INFO scripts.py:1098 -- 14/65 stopped.2024-04-04 22:51:55,327	INFO scripts.py:1098 -- 15/65 stopped.2024-04-04 22:51:55,327	INFO scripts.py:1098 -- 16/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 17/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 18/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 19/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 20/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 21/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 22/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 23/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 24/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 25/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 26/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 27/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 28/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 29/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 30/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 31/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 32/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 33/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 34/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 35/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 36/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 37/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 38/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 39/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 40/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 41/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 42/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 43/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 44/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 45/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 46/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 47/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 48/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 49/65 stopped.2024-04-04 22:51:55,367	INFO scripts.py:1098 -- 50/65 stopped.2024-04-04 22:51:55,419	INFO scripts.py:1098 -- 51/65 stopped.2024-04-04 22:51:55,419	INFO scripts.py:1098 -- 52/65 stopped.2024-04-04 22:51:55,419	INFO scripts.py:1098 -- 53/65 stopped.2024-04-04 22:51:55,420	INFO scripts.py:1098 -- 54/65 stopped.2024-04-04 22:51:55,420	INFO scripts.py:1098 -- 55/65 stopped.2024-04-04 22:51:55,420	INFO scripts.py:1098 -- 56/65 stopped.2024-04-04 22:51:55,420	INFO scripts.py:1098 -- 57/65 stopped.2024-04-04 22:51:55,420	INFO scripts.py:1098 -- 58/65 stopped.2024-04-04 22:51:55,420	INFO scripts.py:1098 -- 59/65 stopped.2024-04-04 22:51:55,420	INFO scripts.py:1098 -- 60/65 stopped.2024-04-04 22:51:55,420	INFO scripts.py:1098 -- 61/65 stopped.2024-04-04 22:51:55,420	INFO scripts.py:1098 -- 62/65 stopped.2024-04-04 22:51:55,793	INFO scripts.py:1098 -- 63/65 stopped.2024-04-04 22:51:56,380	INFO scripts.py:1098 -- 64/65 stopped.2024-04-04 22:51:56,380	INFO scripts.py:1098 -- 65/65 stopped.2024-04-04 22:51:56,479	VINFO scripts.py:1068 -- Send termination request to `[1m/p/scratch/laionize/cache-kun1/miniconda3/envs/ray_2.6/lib/python3.9/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2024-04-04_22-39-03_353741_14396/logs --config_list=eyJvYmplY3Rfc3BpbGxpbmdfY29uZmlnIjogIntcInR5cGVcIjogXCJmaWxlc3lzdGVtXCIsIFwicGFyYW1zXCI6IHtcImRpcmVjdG9yeV9wYXRoXCI6IFwiL3RtcC9yYXkvc2Vzc2lvbl8yMDI0LTA0LTA0XzIyLTM5LTAzXzM1Mzc0MV8xNDM5NlwifX0iLCAiaXNfZXh0ZXJuYWxfc3RvcmFnZV90eXBlX2ZzIjogdHJ1ZX0= --gcs_server_port=6379 --metrics-agent-port=46040 --node-ip-address=10.13.1.34 --session-name=session_2024-04-04_22-39-03_353741_14396[22m[26m` [2m(via SIGTERM)[22m[26m
2024-04-04 22:51:57,374	INFO scripts.py:1098 -- 1/1 stopped.2024-04-04 22:51:57,374	SUCC scripts.py:1142 -- [32mStopped all 66 Ray processes.[39m
